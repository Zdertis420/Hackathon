# Что нужно сделать?
1. Разработать алгоритма определения значимости слов:
   Взять все документы и найти кучу повторяющихся слов паразитов, затем убрать их из документов. 
   Оставшиеся большинство слов в каждом конкретном документе и будут самые значимые слова документа.
2. Сформулировать что такое похожие документы, и реализовать функцию сходства:
   Чем больше документы имеют похожих значимых слов, тем больше документы похожи между собой по теме.
3. Разработать алгоритм классификации статей по темам:
   Похожие ключевые слова = похожие темы
4. Интегрировать все части в единое средство классификации.

# Как мы будем это делать??
## Задача 1
0. читаем все файлы в оперативку
1. убираем знаки препинания и сплитаем и приводим в один регистр
2. токенизация (хэш-мап, в котором индекс - токен, значение - количество)
3. векторизация (представим каждый файл как вектор в N-мерном пр-ве, где N - число слов)
4. нормализация всех векторов (xi /= sqrt(xi^2 | i = [0..N])) (делим вектора на их длину)
5. смотрим, какие слова имеют примерно одинаковую конц-ию (смотрим по измерениям векторов)
6. удаляем незначащие слова (те, которые мы нашли в 5 пункте)
7. выводим оставшееся в файл
## Задача 2
1. векторизуем+нормализуем данные из файлов, которые получили в 1 задаче
2. считаем угол между векторами: это и будет коэффициент сходства
3. выводим это в соотв. файлы
## Задача 3:
1. пользуемся данными из задачи 2 и просто находим тему, с которой имеем макс. сходство
